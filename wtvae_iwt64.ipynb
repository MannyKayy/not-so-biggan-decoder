{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4228655\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Create a folder called celeba in home dir where reconstructed images will be stored\n",
    "#Considered only 100000 images for training\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as utils\n",
    "import gc\n",
    "import pywt\n",
    "import IPython\n",
    "import random\n",
    "from random import sample\n",
    "with __import__('importnb').Notebook():\n",
    "        from wtvae_wt64 import CelebaDataset, VAE, Flatten\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "CUDA = True\n",
    "BATCH_SIZE = 32\n",
    "LOG_INTERVAL = 5\n",
    "h_img = 64\n",
    "w_img = 64\n",
    "flat = h_img*w_img*3\n",
    "train_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w=pywt.Wavelet('bior2.2')\n",
    "\n",
    "\n",
    "dec_hi = torch.Tensor(w.dec_hi[::-1]).cuda() \n",
    "dec_lo = torch.Tensor(w.dec_lo[::-1]).cuda()\n",
    "rec_hi = torch.Tensor(w.rec_hi).cuda()\n",
    "rec_lo = torch.Tensor(w.rec_lo).cuda()\n",
    "\n",
    "inv_filters = torch.stack([rec_lo.unsqueeze(0)*rec_lo.unsqueeze(1),\n",
    "                           rec_lo.unsqueeze(0)*rec_hi.unsqueeze(1),\n",
    "                           rec_hi.unsqueeze(0)*rec_lo.unsqueeze(1),\n",
    "                           rec_hi.unsqueeze(0)*rec_hi.unsqueeze(1)], dim=0)\n",
    "\n",
    "def iwt(vres, levels=1):\n",
    "    h = vres.size(2)\n",
    "    w = vres.size(3)\n",
    "    res = vres.contiguous().view(-1,h//2,2,w//2).transpose(1,2).contiguous().view(-1,4,h//2,w//2).clone()\n",
    "    print(res.shape)\n",
    "    if levels>1:\n",
    "        res[:,:1] = iwt(res[:,:1], levels=levels-1)\n",
    "    res = torch.nn.functional.conv_transpose2d(res, Variable(inv_filters[:,None]),stride=2)\n",
    "    res = res[:,:,2:-2,2:-2] #removing padding\n",
    "#     print(res.shape)\n",
    "    return res\n",
    "\n",
    "def truncated_normal_(tensor, mean=0, std=0.02):\n",
    "    size = tensor.shape\n",
    "    tmp = tensor.new_empty(size + (4,)).normal_()\n",
    "    valid = (tmp < 2) & (tmp > -2)\n",
    "    ind = valid.max(-1, keepdim=True)[1]\n",
    "    tensor.data.copy_(tmp.gather(-1, ind).squeeze(-1))\n",
    "    tensor.data.mul_(std).add_(mean)\n",
    "    \n",
    "def weights_init(m):\n",
    "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "        with torch.no_grad():\n",
    "            truncated_normal_(m.weight.data, mean=0, std=0.02)\n",
    "        nn.init.constant_(m.bias.data, 0.0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight.data, mean=0, std=0.02)\n",
    "        nn.init.constant_(m.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnIWT(nn.Module):\n",
    "    def __init__(self, image_channels=3, z_dim=100, device=None):\n",
    "        super(LearnIWT, self).__init__()\n",
    "        \n",
    "        if device is None:\n",
    "            self.cuda = False\n",
    "            self.device = None\n",
    "        else:\n",
    "            self.device = device\n",
    "            self.cuda = True\n",
    "        \n",
    "        self.z_dim = z_dim\n",
    "        self.leakyrelu = nn.LeakyReLU(0.2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # X - Y Residual Encoder\n",
    "        self.e1 = nn.Conv2d(3, 64, 4, stride=2, padding=1, bias=True, padding_mode='zeros') #[b, 64, 32, 32]\n",
    "        weights_init(self.e1)\n",
    "        self.instance_norm_e1 = nn.InstanceNorm2d(num_features=64, affine=False)\n",
    "\n",
    "        self.e2 = nn.Conv2d(64, 128, 4, stride=2, padding=1, bias=True, padding_mode='zeros') #[b, 128, 16, 16]\n",
    "        weights_init(self.e2)\n",
    "        self.instance_norm_e2 = nn.InstanceNorm2d(num_features=128, affine=False)\n",
    "\n",
    "        self.e3 = nn.Conv2d(128, 256, 4, stride=2, padding=1, bias=True, padding_mode='zeros') #[b, 256, 8, 8]\n",
    "        weights_init(self.e3)\n",
    "        self.instance_norm_e3 = nn.InstanceNorm2d(num_features=256, affine=False)\n",
    "\n",
    "        self.e4 = nn.Conv2d(256, 512, 4, stride=2, padding=1, bias=True, padding_mode='zeros') #[b, 512, 4, 4]\n",
    "        weights_init(self.e4)\n",
    "        self.instance_norm_e4 = nn.InstanceNorm2d(num_features=512, affine=False)\n",
    "        \n",
    "        self.fc_enc = nn.Linear(512 * 4 * 4, 256)\n",
    "        weights_init(self.fc_enc)\n",
    "        \n",
    "        self.fc_mean = nn.Linear(256, z_dim)\n",
    "        weights_init(self.fc_mean)\n",
    "        \n",
    "        self.fc_var = nn.Linear(256, z_dim)\n",
    "        weights_init(self.fc_var)\n",
    "        \n",
    "        # IWT Decoder        \n",
    "#         self.d1 = nn.Linear(3 * 64 * 64, 3 * 64 * 64)\n",
    "        self.d1 = nn.Conv1d(3, 3, kernel_size=1, stride=1)\n",
    "        weights_init(self.d1)\n",
    "        self.mu1 = nn.Linear(z_dim, 3 * 64 * 64)\n",
    "        self.var1 = nn.Linear(z_dim, 3 * 64 * 64)\n",
    "        self.instance_norm_d1 = nn.InstanceNorm2d(num_features=3, affine=False)\n",
    "        self.iwt1 = nn.ConvTranspose2d(image_channels, image_channels, kernel_size=5, stride=1, padding=2)\n",
    "        \n",
    "#         self.d2 = nn.Linear(3 * 64 * 64, 3 * 64 * 64)\n",
    "#         weights_init(self.d2)\n",
    "        self.d2 = nn.Conv1d(3, 3, kernel_size=1, stride=1)\n",
    "        weights_init(self.d1)\n",
    "        self.mu2 = nn.Linear(z_dim, 3 * 64 * 64)\n",
    "        self.var2 = nn.Linear(z_dim, 3 * 64 * 64)\n",
    "        self.instance_norm_d2 = nn.InstanceNorm2d(num_features=3, affine=False)\n",
    "        self.iwt2 = nn.ConvTranspose2d(image_channels, image_channels, kernel_size=5, stride=1, padding=2)\n",
    "        \n",
    "    \n",
    "    def encode(self, x, y):\n",
    "        h = self.leakyrelu(self.instance_norm_e1(self.e1(x-y)))   #[b, 64, 32, 32]\n",
    "        h = self.leakyrelu(self.instance_norm_e2(self.e2(h)))     #[b, 128, 16, 16]\n",
    "        h = self.leakyrelu(self.instance_norm_e3(self.e3(h)))     #[b, 256, 8, 8]\n",
    "        h = self.leakyrelu(self.instance_norm_e4(self.e4(h)))     #[b, 512, 4, 4]\n",
    "        h = self.leakyrelu(self.fc_enc(h.view(-1,512*4*4)))       #[b, 512 * 4 * 4]\n",
    "        \n",
    "        return self.fc_mean(h), F.softplus(self.fc_var(h))        #[b, z_dim]\n",
    "\n",
    "    def reparameterize(self, mu, var):\n",
    "        std = torch.sqrt(var)\n",
    "        if self.cuda:\n",
    "            eps = torch.FloatTensor(std.size()).normal_().to(self.device)\n",
    "        else:\n",
    "            eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu) \n",
    "    \n",
    "    def decode(self, y, z):\n",
    "        mu = self.mu1(z).reshape(-1, 3, 64, 64)\n",
    "        var = self.var1(z).reshape(-1, 3, 64, 64)\n",
    "        h = self.leakyrelu(var*self.instance_norm_d1(self.d1(y.view(y.shape[0], 3, 64*64)).reshape(-1, 3, 64, 64)) + mu) #[b, 3, 64, 64]\n",
    "        h = self.leakyrelu(self.iwt1(h))                               #[b, 3, 64, 64]\n",
    "        \n",
    "        mu = self.mu2(z).reshape(-1, 3, 64, 64)\n",
    "        var = self.var2(z).reshape(-1, 3, 64, 64)\n",
    "        h = self.leakyrelu(var*self.instance_norm_d2(self.d2(h.view(h.shape[0], 3, 64*64)).reshape(-1, 3, 64, 64)) + mu) #[b, 3, 64, 64]\n",
    "        h = self.leakyrelu(self.iwt2(h))                               #[b, 3, 64, 64]\n",
    "        \n",
    "        return self.sigmoid(h)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        mu, var = self.encode(x, y)\n",
    "        if self.training:\n",
    "            z = self.reparameterize(mu, var)\n",
    "        else:\n",
    "            z = mu\n",
    "        x_hat = self.decode(y, z)\n",
    "        \n",
    "        return x_hat, mu, var\n",
    "        \n",
    "        \n",
    "    def loss_function(self, x, x_hat, mu, var) -> Variable:\n",
    "        \n",
    "        # Loss btw reconstructed img and original img\n",
    "        BCE = F.mse_loss(x_hat.view(-1), x.view(-1))\n",
    "        \n",
    "        logvar = torch.log(var)\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) * 0.01\n",
    "#         KLD /= x.shape[0] * 3 * 64 * 64\n",
    "\n",
    "        return BCE + KLD\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for VAE:\n\tMissing key(s) in state_dict: \"wt3.0.weight\", \"wt3.0.bias\", \"wt3.1.weight\", \"wt3.1.bias\", \"wt3.1.running_mean\", \"wt3.1.running_var\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b97edaeeb01e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mwt_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mwt_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/disk_c/han'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/models/wtvae64_2wt_models/wtvae_epoch82.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mwt_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwt_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/disk_c/han/anaconda3/envs/wtvae/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 830\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    831\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for VAE:\n\tMissing key(s) in state_dict: \"wt3.0.weight\", \"wt3.0.bias\", \"wt3.1.weight\", \"wt3.1.bias\", \"wt3.1.running_mean\", \"wt3.1.running_var\". "
     ]
    }
   ],
   "source": [
    "# model = LearnIWT()\n",
    "\n",
    "# sum(x.numel() for x in model.parameters())\n",
    "\n",
    "wt_model = VAE()\n",
    "wt_model.load_state_dict(torch.load('/disk_c/han' + '/models/wtvae64_2wt_models/wtvae_epoch82.pth'))\n",
    "wt_model.eval()\n",
    "sum(x.numel() for x in wt_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = torch.zeros((32,3,32,32))\n",
    "# test1 = test.contiguous().view(-1,32//2,2,32//2).transpose(1,2).contiguous().view(-1,4,32//2,32//2).clone()\n",
    "# print(test1.shape)\n",
    "# #torch.nn.functional.conv_transpose2d(test1.cuda(), Variable(inv_filters[:,None]),stride=2)\n",
    "# t = nn.ConvTranspose2d(3, 3, kernel_size=6, stride=2)\n",
    "# t1 = Variable(inv_filters[:,None])\n",
    "# print(t1.shape)\n",
    "# print(t.weight.shape)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     t.weight = torch.nn.Parameter(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create celeba dataset\n",
    "root_dir = '/disk_c/han/'\n",
    "img_dir = os.path.join(root_dir, 'celeba64/')\n",
    "image_files = os.listdir(img_dir)\n",
    "train_dataset = CelebaDataset(img_dir, image_files, WT=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=10, shuffle=True)\n",
    "sample_dataset = Subset(train_dataset, sample(range(len(train_dataset)), 8))\n",
    "sample_loader = DataLoader(sample_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zeroing out all other patches than the first for WT image: 4D: B * C * H * W\n",
    "def zero_patches(img):\n",
    "    zeros = torch.zeros((img.shape[0], img.shape[1], img.shape[2], img.shape[3]))\n",
    "    zeros[:,:,:16,:16] = img[:,:,:16,:16]\n",
    "    \n",
    "    return zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer, train_loader):\n",
    "    # toggle model to train mode\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        \n",
    "        if CUDA:\n",
    "            data0 = data.to('cuda:0')\n",
    "            data1 = data.clone().to('cuda:1')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Get Y\n",
    "        Y = wt_model(data1)[0]\n",
    "        # Zeroing out all other patches\n",
    "        Y = zero_patches(Y)\n",
    "        x_hat, mu, var = iwt_model(data0, Y.to('cuda:0'))\n",
    "        # Fix loss function\n",
    "        loss = iwt_model.loss_function(x_hat, data0, mu, var)\n",
    "        loss.backward()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        train_loss += loss\n",
    "        optimizer.step()\n",
    "        if batch_idx % LOG_INTERVAL == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(data),\n",
    "                                                                           len(train_loader.dataset),\n",
    "                                                                           100. * batch_idx / len(train_loader),\n",
    "                                                                           loss / len(data)))\n",
    "            \n",
    "            n = min(data.size(0), 8)  \n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/10000 (0%)]\tLoss: 0.081471\n",
      "Train Epoch: 1 [160/10000 (2%)]\tLoss: 0.035353\n",
      "Train Epoch: 1 [320/10000 (3%)]\tLoss: 0.010042\n",
      "Train Epoch: 1 [480/10000 (5%)]\tLoss: 0.006518\n",
      "Train Epoch: 1 [640/10000 (6%)]\tLoss: 0.004635\n",
      "Train Epoch: 1 [800/10000 (8%)]\tLoss: 0.004133\n",
      "Train Epoch: 1 [960/10000 (10%)]\tLoss: 0.003762\n",
      "Train Epoch: 1 [1120/10000 (11%)]\tLoss: 0.003143\n",
      "Train Epoch: 1 [1280/10000 (13%)]\tLoss: 0.003186\n",
      "Train Epoch: 1 [1440/10000 (14%)]\tLoss: 0.002831\n",
      "Train Epoch: 1 [1600/10000 (16%)]\tLoss: 0.002696\n",
      "Train Epoch: 1 [1760/10000 (18%)]\tLoss: 0.002951\n",
      "Train Epoch: 1 [1920/10000 (19%)]\tLoss: 0.002771\n",
      "Train Epoch: 1 [2080/10000 (21%)]\tLoss: 0.003282\n",
      "Train Epoch: 1 [2240/10000 (22%)]\tLoss: 0.002703\n",
      "Train Epoch: 1 [2400/10000 (24%)]\tLoss: 0.002514\n",
      "Train Epoch: 1 [2560/10000 (26%)]\tLoss: 0.002576\n",
      "Train Epoch: 1 [2720/10000 (27%)]\tLoss: 0.002875\n",
      "Train Epoch: 1 [2880/10000 (29%)]\tLoss: 0.002526\n",
      "Train Epoch: 1 [3040/10000 (30%)]\tLoss: 0.002384\n",
      "Train Epoch: 1 [3200/10000 (32%)]\tLoss: 0.002893\n",
      "Train Epoch: 1 [3360/10000 (34%)]\tLoss: 0.002542\n",
      "Train Epoch: 1 [3520/10000 (35%)]\tLoss: 0.002653\n",
      "Train Epoch: 1 [3680/10000 (37%)]\tLoss: 0.002491\n",
      "Train Epoch: 1 [3840/10000 (38%)]\tLoss: 0.002747\n",
      "Train Epoch: 1 [4000/10000 (40%)]\tLoss: 0.002684\n",
      "Train Epoch: 1 [4160/10000 (42%)]\tLoss: 0.002965\n",
      "Train Epoch: 1 [4320/10000 (43%)]\tLoss: 0.002286\n",
      "Train Epoch: 1 [4480/10000 (45%)]\tLoss: 0.002926\n",
      "Train Epoch: 1 [4640/10000 (46%)]\tLoss: 0.002594\n",
      "Train Epoch: 1 [4800/10000 (48%)]\tLoss: 0.002463\n",
      "Train Epoch: 1 [4960/10000 (50%)]\tLoss: 0.002870\n",
      "Train Epoch: 1 [5120/10000 (51%)]\tLoss: 0.002655\n",
      "Train Epoch: 1 [5280/10000 (53%)]\tLoss: 0.002284\n",
      "Train Epoch: 1 [5440/10000 (54%)]\tLoss: 0.002521\n",
      "Train Epoch: 1 [5600/10000 (56%)]\tLoss: 0.002285\n",
      "Train Epoch: 1 [5760/10000 (58%)]\tLoss: 0.002234\n",
      "Train Epoch: 1 [5920/10000 (59%)]\tLoss: 0.002532\n",
      "Train Epoch: 1 [6080/10000 (61%)]\tLoss: 0.002771\n",
      "Train Epoch: 1 [6240/10000 (62%)]\tLoss: 0.002643\n",
      "Train Epoch: 1 [6400/10000 (64%)]\tLoss: 0.002639\n",
      "Train Epoch: 1 [6560/10000 (65%)]\tLoss: 0.002333\n",
      "Train Epoch: 1 [6720/10000 (67%)]\tLoss: 0.002724\n",
      "Train Epoch: 1 [6880/10000 (69%)]\tLoss: 0.002498\n",
      "Train Epoch: 1 [7040/10000 (70%)]\tLoss: 0.002764\n",
      "Train Epoch: 1 [7200/10000 (72%)]\tLoss: 0.002626\n",
      "Train Epoch: 1 [7360/10000 (73%)]\tLoss: 0.002339\n",
      "Train Epoch: 1 [7520/10000 (75%)]\tLoss: 0.002696\n",
      "Train Epoch: 1 [7680/10000 (77%)]\tLoss: 0.002502\n",
      "Train Epoch: 1 [7840/10000 (78%)]\tLoss: 0.002286\n",
      "Train Epoch: 1 [8000/10000 (80%)]\tLoss: 0.002414\n",
      "Train Epoch: 1 [8160/10000 (81%)]\tLoss: 0.002676\n",
      "Train Epoch: 1 [8320/10000 (83%)]\tLoss: 0.002697\n",
      "Train Epoch: 1 [8480/10000 (85%)]\tLoss: 0.002628\n",
      "Train Epoch: 1 [8640/10000 (86%)]\tLoss: 0.002532\n",
      "Train Epoch: 1 [8800/10000 (88%)]\tLoss: 0.002386\n",
      "Train Epoch: 1 [8960/10000 (89%)]\tLoss: 0.002431\n",
      "Train Epoch: 1 [9120/10000 (91%)]\tLoss: 0.002195\n",
      "Train Epoch: 1 [9280/10000 (93%)]\tLoss: 0.002588\n",
      "Train Epoch: 1 [9440/10000 (94%)]\tLoss: 0.002832\n",
      "Train Epoch: 1 [9600/10000 (96%)]\tLoss: 0.002475\n",
      "Train Epoch: 1 [9760/10000 (97%)]\tLoss: 0.002471\n",
      "Train Epoch: 1 [9920/10000 (99%)]\tLoss: 0.002813\n",
      "====> Epoch: 1 Average loss: 0.0051\n",
      "Train Epoch: 2 [0/10000 (0%)]\tLoss: 0.002728\n",
      "Train Epoch: 2 [160/10000 (2%)]\tLoss: 0.002175\n",
      "Train Epoch: 2 [320/10000 (3%)]\tLoss: 0.002503\n",
      "Train Epoch: 2 [480/10000 (5%)]\tLoss: 0.002679\n",
      "Train Epoch: 2 [640/10000 (6%)]\tLoss: 0.002414\n",
      "Train Epoch: 2 [800/10000 (8%)]\tLoss: 0.002679\n",
      "Train Epoch: 2 [960/10000 (10%)]\tLoss: 0.002359\n",
      "Train Epoch: 2 [1120/10000 (11%)]\tLoss: 0.002579\n",
      "Train Epoch: 2 [1280/10000 (13%)]\tLoss: 0.002637\n",
      "Train Epoch: 2 [1440/10000 (14%)]\tLoss: 0.002661\n",
      "Train Epoch: 2 [1600/10000 (16%)]\tLoss: 0.002303\n",
      "Train Epoch: 2 [1760/10000 (18%)]\tLoss: 0.002682\n",
      "Train Epoch: 2 [1920/10000 (19%)]\tLoss: 0.002454\n",
      "Train Epoch: 2 [2080/10000 (21%)]\tLoss: 0.002560\n",
      "Train Epoch: 2 [2240/10000 (22%)]\tLoss: 0.002321\n",
      "Train Epoch: 2 [2400/10000 (24%)]\tLoss: 0.002516\n",
      "Train Epoch: 2 [2560/10000 (26%)]\tLoss: 0.002273\n",
      "Train Epoch: 2 [2720/10000 (27%)]\tLoss: 0.002642\n",
      "Train Epoch: 2 [2880/10000 (29%)]\tLoss: 0.002526\n",
      "Train Epoch: 2 [3040/10000 (30%)]\tLoss: 0.002534\n",
      "Train Epoch: 2 [3200/10000 (32%)]\tLoss: 0.002742\n",
      "Train Epoch: 2 [3360/10000 (34%)]\tLoss: 0.002592\n",
      "Train Epoch: 2 [3520/10000 (35%)]\tLoss: 0.002348\n",
      "Train Epoch: 2 [3680/10000 (37%)]\tLoss: 0.002388\n",
      "Train Epoch: 2 [3840/10000 (38%)]\tLoss: 0.002446\n",
      "Train Epoch: 2 [4000/10000 (40%)]\tLoss: 0.002541\n",
      "Train Epoch: 2 [4160/10000 (42%)]\tLoss: 0.002583\n",
      "Train Epoch: 2 [4320/10000 (43%)]\tLoss: 0.002699\n",
      "Train Epoch: 2 [4480/10000 (45%)]\tLoss: 0.002472\n",
      "Train Epoch: 2 [4640/10000 (46%)]\tLoss: 0.001977\n",
      "Train Epoch: 2 [4800/10000 (48%)]\tLoss: 0.002554\n",
      "Train Epoch: 2 [4960/10000 (50%)]\tLoss: 0.002223\n",
      "Train Epoch: 2 [5120/10000 (51%)]\tLoss: 0.002479\n",
      "Train Epoch: 2 [5280/10000 (53%)]\tLoss: 0.002442\n",
      "Train Epoch: 2 [5440/10000 (54%)]\tLoss: 0.002365\n",
      "Train Epoch: 2 [5600/10000 (56%)]\tLoss: 0.002548\n",
      "Train Epoch: 2 [5760/10000 (58%)]\tLoss: 0.002538\n",
      "Train Epoch: 2 [5920/10000 (59%)]\tLoss: 0.002546\n",
      "Train Epoch: 2 [6080/10000 (61%)]\tLoss: 0.002749\n",
      "Train Epoch: 2 [6240/10000 (62%)]\tLoss: 0.002423\n",
      "Train Epoch: 2 [6400/10000 (64%)]\tLoss: 0.002331\n",
      "Train Epoch: 2 [6560/10000 (65%)]\tLoss: 0.002353\n",
      "Train Epoch: 2 [6720/10000 (67%)]\tLoss: 0.002532\n",
      "Train Epoch: 2 [6880/10000 (69%)]\tLoss: 0.002532\n",
      "Train Epoch: 2 [7040/10000 (70%)]\tLoss: 0.002252\n",
      "Train Epoch: 2 [7200/10000 (72%)]\tLoss: 0.002444\n",
      "Train Epoch: 2 [7360/10000 (73%)]\tLoss: 0.002351\n",
      "Train Epoch: 2 [7520/10000 (75%)]\tLoss: 0.002471\n",
      "Train Epoch: 2 [7680/10000 (77%)]\tLoss: 0.002535\n",
      "Train Epoch: 2 [7840/10000 (78%)]\tLoss: 0.002212\n",
      "Train Epoch: 2 [8000/10000 (80%)]\tLoss: 0.002721\n",
      "Train Epoch: 2 [8160/10000 (81%)]\tLoss: 0.002471\n",
      "Train Epoch: 2 [8320/10000 (83%)]\tLoss: 0.002328\n",
      "Train Epoch: 2 [8480/10000 (85%)]\tLoss: 0.002826\n",
      "Train Epoch: 2 [8640/10000 (86%)]\tLoss: 0.002629\n",
      "Train Epoch: 2 [8800/10000 (88%)]\tLoss: 0.002344\n",
      "Train Epoch: 2 [8960/10000 (89%)]\tLoss: 0.002435\n",
      "Train Epoch: 2 [9120/10000 (91%)]\tLoss: 0.002635\n",
      "Train Epoch: 2 [9280/10000 (93%)]\tLoss: 0.002326\n",
      "Train Epoch: 2 [9440/10000 (94%)]\tLoss: 0.002477\n",
      "Train Epoch: 2 [9600/10000 (96%)]\tLoss: 0.002408\n",
      "Train Epoch: 2 [9760/10000 (97%)]\tLoss: 0.002629\n",
      "Train Epoch: 2 [9920/10000 (99%)]\tLoss: 0.002329\n",
      "====> Epoch: 2 Average loss: 0.0025\n",
      "Train Epoch: 3 [0/10000 (0%)]\tLoss: 0.002570\n",
      "Train Epoch: 3 [160/10000 (2%)]\tLoss: 0.002608\n",
      "Train Epoch: 3 [320/10000 (3%)]\tLoss: 0.002731\n",
      "Train Epoch: 3 [480/10000 (5%)]\tLoss: 0.002746\n",
      "Train Epoch: 3 [640/10000 (6%)]\tLoss: 0.002625\n",
      "Train Epoch: 3 [800/10000 (8%)]\tLoss: 0.002429\n",
      "Train Epoch: 3 [960/10000 (10%)]\tLoss: 0.002262\n",
      "Train Epoch: 3 [1120/10000 (11%)]\tLoss: 0.002039\n",
      "Train Epoch: 3 [1280/10000 (13%)]\tLoss: 0.002553\n",
      "Train Epoch: 3 [1440/10000 (14%)]\tLoss: 0.002319\n",
      "Train Epoch: 3 [1600/10000 (16%)]\tLoss: 0.002303\n",
      "Train Epoch: 3 [1760/10000 (18%)]\tLoss: 0.002278\n",
      "Train Epoch: 3 [1920/10000 (19%)]\tLoss: 0.002323\n",
      "Train Epoch: 3 [2080/10000 (21%)]\tLoss: 0.002270\n",
      "Train Epoch: 3 [2240/10000 (22%)]\tLoss: 0.002131\n",
      "Train Epoch: 3 [2400/10000 (24%)]\tLoss: 0.002320\n",
      "Train Epoch: 3 [2560/10000 (26%)]\tLoss: 0.002449\n",
      "Train Epoch: 3 [2720/10000 (27%)]\tLoss: 0.002553\n",
      "Train Epoch: 3 [2880/10000 (29%)]\tLoss: 0.002823\n",
      "Train Epoch: 3 [3040/10000 (30%)]\tLoss: 0.002498\n",
      "Train Epoch: 3 [3200/10000 (32%)]\tLoss: 0.002635\n",
      "Train Epoch: 3 [3360/10000 (34%)]\tLoss: 0.002604\n",
      "Train Epoch: 3 [3520/10000 (35%)]\tLoss: 0.002176\n",
      "Train Epoch: 3 [3680/10000 (37%)]\tLoss: 0.002364\n",
      "Train Epoch: 3 [3840/10000 (38%)]\tLoss: 0.002288\n",
      "Train Epoch: 3 [4000/10000 (40%)]\tLoss: 0.002445\n",
      "Train Epoch: 3 [4160/10000 (42%)]\tLoss: 0.002689\n",
      "Train Epoch: 3 [4320/10000 (43%)]\tLoss: 0.002178\n",
      "Train Epoch: 3 [4480/10000 (45%)]\tLoss: 0.002357\n",
      "Train Epoch: 3 [4640/10000 (46%)]\tLoss: 0.002216\n",
      "Train Epoch: 3 [4800/10000 (48%)]\tLoss: 0.002321\n",
      "Train Epoch: 3 [4960/10000 (50%)]\tLoss: 0.002329\n",
      "Train Epoch: 3 [5120/10000 (51%)]\tLoss: 0.002326\n",
      "Train Epoch: 3 [5280/10000 (53%)]\tLoss: 0.002309\n",
      "Train Epoch: 3 [5440/10000 (54%)]\tLoss: 0.002234\n",
      "Train Epoch: 3 [5600/10000 (56%)]\tLoss: 0.002217\n",
      "Train Epoch: 3 [5760/10000 (58%)]\tLoss: 0.002170\n",
      "Train Epoch: 3 [5920/10000 (59%)]\tLoss: 0.002080\n",
      "Train Epoch: 3 [6080/10000 (61%)]\tLoss: 0.002499\n",
      "Train Epoch: 3 [6240/10000 (62%)]\tLoss: 0.002350\n",
      "Train Epoch: 3 [6400/10000 (64%)]\tLoss: 0.002751\n",
      "Train Epoch: 3 [6560/10000 (65%)]\tLoss: 0.002434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [6720/10000 (67%)]\tLoss: 0.002025\n",
      "Train Epoch: 3 [6880/10000 (69%)]\tLoss: 0.002524\n",
      "Train Epoch: 3 [7040/10000 (70%)]\tLoss: 0.002490\n",
      "Train Epoch: 3 [7200/10000 (72%)]\tLoss: 0.001853\n",
      "Train Epoch: 3 [7360/10000 (73%)]\tLoss: 0.002971\n",
      "Train Epoch: 3 [7520/10000 (75%)]\tLoss: 0.002310\n",
      "Train Epoch: 3 [7680/10000 (77%)]\tLoss: 0.002502\n",
      "Train Epoch: 3 [7840/10000 (78%)]\tLoss: 0.002515\n",
      "Train Epoch: 3 [8000/10000 (80%)]\tLoss: 0.002153\n",
      "Train Epoch: 3 [8160/10000 (81%)]\tLoss: 0.001912\n",
      "Train Epoch: 3 [8320/10000 (83%)]\tLoss: 0.002312\n",
      "Train Epoch: 3 [8480/10000 (85%)]\tLoss: 0.002443\n",
      "Train Epoch: 3 [8640/10000 (86%)]\tLoss: 0.002345\n",
      "Train Epoch: 3 [8800/10000 (88%)]\tLoss: 0.002321\n",
      "Train Epoch: 3 [8960/10000 (89%)]\tLoss: 0.002105\n",
      "Train Epoch: 3 [9120/10000 (91%)]\tLoss: 0.002352\n",
      "Train Epoch: 3 [9280/10000 (93%)]\tLoss: 0.002207\n",
      "Train Epoch: 3 [9440/10000 (94%)]\tLoss: 0.002154\n",
      "Train Epoch: 3 [9600/10000 (96%)]\tLoss: 0.002184\n",
      "Train Epoch: 3 [9760/10000 (97%)]\tLoss: 0.002363\n",
      "Train Epoch: 3 [9920/10000 (99%)]\tLoss: 0.002256\n",
      "====> Epoch: 3 Average loss: 0.0024\n",
      "Train Epoch: 4 [0/10000 (0%)]\tLoss: 0.002169\n",
      "Train Epoch: 4 [160/10000 (2%)]\tLoss: 0.002345\n",
      "Train Epoch: 4 [320/10000 (3%)]\tLoss: 0.002507\n",
      "Train Epoch: 4 [480/10000 (5%)]\tLoss: 0.002185\n",
      "Train Epoch: 4 [640/10000 (6%)]\tLoss: 0.002175\n",
      "Train Epoch: 4 [800/10000 (8%)]\tLoss: 0.002238\n",
      "Train Epoch: 4 [960/10000 (10%)]\tLoss: 0.002218\n",
      "Train Epoch: 4 [1120/10000 (11%)]\tLoss: 0.002279\n",
      "Train Epoch: 4 [1280/10000 (13%)]\tLoss: 0.002217\n",
      "Train Epoch: 4 [1440/10000 (14%)]\tLoss: 0.002158\n",
      "Train Epoch: 4 [1600/10000 (16%)]\tLoss: 0.002345\n",
      "Train Epoch: 4 [1760/10000 (18%)]\tLoss: 0.002487\n",
      "Train Epoch: 4 [1920/10000 (19%)]\tLoss: 0.002407\n",
      "Train Epoch: 4 [2080/10000 (21%)]\tLoss: 0.002082\n",
      "Train Epoch: 4 [2240/10000 (22%)]\tLoss: 0.002361\n",
      "Train Epoch: 4 [2400/10000 (24%)]\tLoss: 0.002642\n",
      "Train Epoch: 4 [2560/10000 (26%)]\tLoss: 0.002154\n",
      "Train Epoch: 4 [2720/10000 (27%)]\tLoss: 0.002105\n",
      "Train Epoch: 4 [2880/10000 (29%)]\tLoss: 0.002049\n",
      "Train Epoch: 4 [3040/10000 (30%)]\tLoss: 0.002346\n",
      "Train Epoch: 4 [3200/10000 (32%)]\tLoss: 0.002073\n",
      "Train Epoch: 4 [3360/10000 (34%)]\tLoss: 0.002300\n",
      "Train Epoch: 4 [3520/10000 (35%)]\tLoss: 0.002281\n",
      "Train Epoch: 4 [3680/10000 (37%)]\tLoss: 0.002268\n",
      "Train Epoch: 4 [3840/10000 (38%)]\tLoss: 0.002170\n",
      "Train Epoch: 4 [4000/10000 (40%)]\tLoss: 0.002333\n",
      "Train Epoch: 4 [4160/10000 (42%)]\tLoss: 0.002304\n",
      "Train Epoch: 4 [4320/10000 (43%)]\tLoss: 0.002190\n",
      "Train Epoch: 4 [4480/10000 (45%)]\tLoss: 0.002261\n",
      "Train Epoch: 4 [4640/10000 (46%)]\tLoss: 0.002066\n",
      "Train Epoch: 4 [4800/10000 (48%)]\tLoss: 0.002287\n",
      "Train Epoch: 4 [4960/10000 (50%)]\tLoss: 0.002241\n",
      "Train Epoch: 4 [5120/10000 (51%)]\tLoss: 0.002633\n",
      "Train Epoch: 4 [5280/10000 (53%)]\tLoss: 0.002407\n",
      "Train Epoch: 4 [5440/10000 (54%)]\tLoss: 0.002357\n",
      "Train Epoch: 4 [5600/10000 (56%)]\tLoss: 0.002079\n",
      "Train Epoch: 4 [5760/10000 (58%)]\tLoss: 0.002098\n",
      "Train Epoch: 4 [5920/10000 (59%)]\tLoss: 0.002100\n",
      "Train Epoch: 4 [6080/10000 (61%)]\tLoss: 0.002207\n",
      "Train Epoch: 4 [6240/10000 (62%)]\tLoss: 0.002035\n",
      "Train Epoch: 4 [6400/10000 (64%)]\tLoss: 0.002231\n",
      "Train Epoch: 4 [6560/10000 (65%)]\tLoss: 0.002078\n",
      "Train Epoch: 4 [6720/10000 (67%)]\tLoss: 0.002182\n",
      "Train Epoch: 4 [6880/10000 (69%)]\tLoss: 0.002143\n",
      "Train Epoch: 4 [7040/10000 (70%)]\tLoss: 0.002073\n",
      "Train Epoch: 4 [7200/10000 (72%)]\tLoss: 0.002210\n",
      "Train Epoch: 4 [7360/10000 (73%)]\tLoss: 0.001903\n",
      "Train Epoch: 4 [7520/10000 (75%)]\tLoss: 0.001991\n",
      "Train Epoch: 4 [7680/10000 (77%)]\tLoss: 0.002159\n",
      "Train Epoch: 4 [7840/10000 (78%)]\tLoss: 0.002241\n",
      "Train Epoch: 4 [8000/10000 (80%)]\tLoss: 0.002245\n",
      "Train Epoch: 4 [8160/10000 (81%)]\tLoss: 0.002058\n",
      "Train Epoch: 4 [8320/10000 (83%)]\tLoss: 0.002409\n",
      "Train Epoch: 4 [8480/10000 (85%)]\tLoss: 0.002301\n",
      "Train Epoch: 4 [8640/10000 (86%)]\tLoss: 0.002127\n",
      "Train Epoch: 4 [8800/10000 (88%)]\tLoss: 0.001894\n",
      "Train Epoch: 4 [8960/10000 (89%)]\tLoss: 0.002333\n",
      "Train Epoch: 4 [9120/10000 (91%)]\tLoss: 0.002130\n",
      "Train Epoch: 4 [9280/10000 (93%)]\tLoss: 0.002202\n",
      "Train Epoch: 4 [9440/10000 (94%)]\tLoss: 0.001994\n",
      "Train Epoch: 4 [9600/10000 (96%)]\tLoss: 0.001921\n",
      "Train Epoch: 4 [9760/10000 (97%)]\tLoss: 0.001996\n",
      "Train Epoch: 4 [9920/10000 (99%)]\tLoss: 0.002079\n",
      "====> Epoch: 4 Average loss: 0.0022\n",
      "Train Epoch: 5 [0/10000 (0%)]\tLoss: 0.002167\n",
      "Train Epoch: 5 [160/10000 (2%)]\tLoss: 0.002288\n",
      "Train Epoch: 5 [320/10000 (3%)]\tLoss: 0.002248\n",
      "Train Epoch: 5 [480/10000 (5%)]\tLoss: 0.002116\n",
      "Train Epoch: 5 [640/10000 (6%)]\tLoss: 0.002207\n",
      "Train Epoch: 5 [800/10000 (8%)]\tLoss: 0.002387\n",
      "Train Epoch: 5 [960/10000 (10%)]\tLoss: 0.002173\n",
      "Train Epoch: 5 [1120/10000 (11%)]\tLoss: 0.002433\n",
      "Train Epoch: 5 [1280/10000 (13%)]\tLoss: 0.002056\n",
      "Train Epoch: 5 [1440/10000 (14%)]\tLoss: 0.002299\n",
      "Train Epoch: 5 [1600/10000 (16%)]\tLoss: 0.002110\n",
      "Train Epoch: 5 [1760/10000 (18%)]\tLoss: 0.002297\n",
      "Train Epoch: 5 [1920/10000 (19%)]\tLoss: 0.002054\n",
      "Train Epoch: 5 [2080/10000 (21%)]\tLoss: 0.001989\n",
      "Train Epoch: 5 [2240/10000 (22%)]\tLoss: 0.002295\n",
      "Train Epoch: 5 [2400/10000 (24%)]\tLoss: 0.002330\n",
      "Train Epoch: 5 [2560/10000 (26%)]\tLoss: 0.002250\n",
      "Train Epoch: 5 [2720/10000 (27%)]\tLoss: 0.001894\n",
      "Train Epoch: 5 [2880/10000 (29%)]\tLoss: 0.002108\n",
      "Train Epoch: 5 [3040/10000 (30%)]\tLoss: 0.002082\n",
      "Train Epoch: 5 [3200/10000 (32%)]\tLoss: 0.002022\n",
      "Train Epoch: 5 [3360/10000 (34%)]\tLoss: 0.001855\n",
      "Train Epoch: 5 [3520/10000 (35%)]\tLoss: 0.002225\n",
      "Train Epoch: 5 [3680/10000 (37%)]\tLoss: 0.001978\n",
      "Train Epoch: 5 [3840/10000 (38%)]\tLoss: 0.002129\n",
      "Train Epoch: 5 [4000/10000 (40%)]\tLoss: 0.002008\n",
      "Train Epoch: 5 [4160/10000 (42%)]\tLoss: 0.002119\n",
      "Train Epoch: 5 [4320/10000 (43%)]\tLoss: 0.002123\n",
      "Train Epoch: 5 [4480/10000 (45%)]\tLoss: 0.002117\n",
      "Train Epoch: 5 [4640/10000 (46%)]\tLoss: 0.002189\n",
      "Train Epoch: 5 [4800/10000 (48%)]\tLoss: 0.002124\n",
      "Train Epoch: 5 [4960/10000 (50%)]\tLoss: 0.001913\n",
      "Train Epoch: 5 [5120/10000 (51%)]\tLoss: 0.002286\n",
      "Train Epoch: 5 [5280/10000 (53%)]\tLoss: 0.002182\n",
      "Train Epoch: 5 [5440/10000 (54%)]\tLoss: 0.002123\n",
      "Train Epoch: 5 [5600/10000 (56%)]\tLoss: 0.001828\n",
      "Train Epoch: 5 [5760/10000 (58%)]\tLoss: 0.002308\n",
      "Train Epoch: 5 [5920/10000 (59%)]\tLoss: 0.002337\n",
      "Train Epoch: 5 [6080/10000 (61%)]\tLoss: 0.001854\n",
      "Train Epoch: 5 [6240/10000 (62%)]\tLoss: 0.002177\n",
      "Train Epoch: 5 [6400/10000 (64%)]\tLoss: 0.001735\n",
      "Train Epoch: 5 [6560/10000 (65%)]\tLoss: 0.002073\n",
      "Train Epoch: 5 [6720/10000 (67%)]\tLoss: 0.001911\n",
      "Train Epoch: 5 [6880/10000 (69%)]\tLoss: 0.002009\n",
      "Train Epoch: 5 [7040/10000 (70%)]\tLoss: 0.002014\n",
      "Train Epoch: 5 [7200/10000 (72%)]\tLoss: 0.002132\n",
      "Train Epoch: 5 [7360/10000 (73%)]\tLoss: 0.001956\n",
      "Train Epoch: 5 [7520/10000 (75%)]\tLoss: 0.001977\n",
      "Train Epoch: 5 [7680/10000 (77%)]\tLoss: 0.001948\n",
      "Train Epoch: 5 [7840/10000 (78%)]\tLoss: 0.002134\n",
      "Train Epoch: 5 [8000/10000 (80%)]\tLoss: 0.001900\n",
      "Train Epoch: 5 [8160/10000 (81%)]\tLoss: 0.002207\n",
      "Train Epoch: 5 [8320/10000 (83%)]\tLoss: 0.001886\n",
      "Train Epoch: 5 [8480/10000 (85%)]\tLoss: 0.002199\n",
      "Train Epoch: 5 [8640/10000 (86%)]\tLoss: 0.002036\n",
      "Train Epoch: 5 [8800/10000 (88%)]\tLoss: 0.002112\n",
      "Train Epoch: 5 [8960/10000 (89%)]\tLoss: 0.001777\n",
      "Train Epoch: 5 [9120/10000 (91%)]\tLoss: 0.002227\n",
      "Train Epoch: 5 [9280/10000 (93%)]\tLoss: 0.001913\n",
      "Train Epoch: 5 [9440/10000 (94%)]\tLoss: 0.001716\n",
      "Train Epoch: 5 [9600/10000 (96%)]\tLoss: 0.002019\n",
      "Train Epoch: 5 [9760/10000 (97%)]\tLoss: 0.001703\n",
      "Train Epoch: 5 [9920/10000 (99%)]\tLoss: 0.002021\n",
      "====> Epoch: 5 Average loss: 0.0021\n",
      "Train Epoch: 6 [0/10000 (0%)]\tLoss: 0.002163\n",
      "Train Epoch: 6 [160/10000 (2%)]\tLoss: 0.002018\n",
      "Train Epoch: 6 [320/10000 (3%)]\tLoss: 0.002153\n",
      "Train Epoch: 6 [480/10000 (5%)]\tLoss: 0.001837\n",
      "Train Epoch: 6 [640/10000 (6%)]\tLoss: 0.002088\n",
      "Train Epoch: 6 [800/10000 (8%)]\tLoss: 0.002135\n",
      "Train Epoch: 6 [960/10000 (10%)]\tLoss: 0.002121\n",
      "Train Epoch: 6 [1120/10000 (11%)]\tLoss: 0.001981\n",
      "Train Epoch: 6 [1280/10000 (13%)]\tLoss: 0.002112\n",
      "Train Epoch: 6 [1440/10000 (14%)]\tLoss: 0.002018\n",
      "Train Epoch: 6 [1600/10000 (16%)]\tLoss: 0.002255\n",
      "Train Epoch: 6 [1760/10000 (18%)]\tLoss: 0.002144\n",
      "Train Epoch: 6 [1920/10000 (19%)]\tLoss: 0.001982\n",
      "Train Epoch: 6 [2080/10000 (21%)]\tLoss: 0.002024\n",
      "Train Epoch: 6 [2240/10000 (22%)]\tLoss: 0.001996\n",
      "Train Epoch: 6 [2400/10000 (24%)]\tLoss: 0.002033\n",
      "Train Epoch: 6 [2560/10000 (26%)]\tLoss: 0.002277\n",
      "Train Epoch: 6 [2720/10000 (27%)]\tLoss: 0.002151\n",
      "Train Epoch: 6 [2880/10000 (29%)]\tLoss: 0.002293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [3040/10000 (30%)]\tLoss: 0.002370\n",
      "Train Epoch: 6 [3200/10000 (32%)]\tLoss: 0.001989\n",
      "Train Epoch: 6 [3360/10000 (34%)]\tLoss: 0.002017\n",
      "Train Epoch: 6 [3520/10000 (35%)]\tLoss: 0.002160\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-85a1a815a920>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miwt_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-0b8854e0b60c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, model, optimizer, train_loader)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Fix loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miwt_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/disk_c/han/anaconda3/envs/wtvae/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/disk_c/han/anaconda3/envs/wtvae/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loading WT model and setting to eval for inference\n",
    "wt_model = VAE()\n",
    "wt_model.load_state_dict(torch.load(root_dir + '/models/wtvae_models_2wt/wtvae_epoch88.pth'))\n",
    "wt_model.to('cuda:1')\n",
    "wt_model.eval()\n",
    "iwt_model = LearnIWT(device='cuda:0').to('cuda:0')\n",
    "\n",
    "postfix = 'rest0_conv1d_2upsample'\n",
    "sample_dir = os.path.join(root_dir, 'image_samples/')\n",
    "model_dir = os.path.join(root_dir, 'models/')\n",
    "if os.path.isdir('/disk_c/han/image_samples/celeba_iwtvae_{}'.format(postfix)) or os.path.isdir('/disk_c/han/models/iwtvae64_models_{}'.format(postfix)):\n",
    "    raise Exception('Image sample / model directory exists!')\n",
    "else:\n",
    "    os.mkdir('/disk_c/han/image_samples/celeba_iwtvae_{}'.format(postfix))\n",
    "    os.mkdir('/disk_c/han/models/iwtvae64_models_{}'.format(postfix))\n",
    "             \n",
    "train_losses = []\n",
    "gc.collect()\n",
    "EPOCHS = 100\n",
    "\n",
    "optimizer = optim.Adam(iwt_model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(epoch, iwt_model, optimizer, train_loader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        iwt_model.eval()\n",
    "\n",
    "        for data in sample_loader:\n",
    "            if CUDA:\n",
    "                data0 = data.to('cuda:0')\n",
    "                data1 = data.to('cuda:1')\n",
    "            \n",
    "            z_sample = torch.randn(data.shape[0],100).to('cuda:0')\n",
    "            \n",
    "            Y = wt_model(data1)[0].to('cuda:0')\n",
    "            mu, var = iwt_model.encode(data0, Y)\n",
    "            x_hat = iwt_model.decode(Y, mu)\n",
    "            x_sample = iwt_model.decode(Y, z_sample)\n",
    "            \n",
    "            save_image(x_hat.cpu(), sample_dir + '/celeba_iwtvae_{}/sample_recon'.format(postfix) + str(epoch) + '.png') \n",
    "            save_image(x_sample.cpu(), sample_dir + '/celeba_iwtvae_{}/sample_z'.format(postfix) + str(epoch) + '.png')\n",
    "            save_image(Y.cpu(), sample_dir + '/celeba_iwtvae_{}/sample_y'.format(postfix) + str(epoch) + '.png') \n",
    "            save_image(data.cpu(), sample_dir + '/celeba_iwtvae_{}/sample'.format(postfix) + str(epoch) + '.png')  \n",
    "    \n",
    "    torch.save(iwt_model.state_dict(), model_dir + '/iwtvae64_models_{}/iwtvae_epoch{}.pth'.format(postfix, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/models'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(root_dir, '/models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/disk_c/han/'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
